{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8: Nearest Neighbors Regression üèò"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Riley Hager\n",
    "\n",
    "Student ID: 455336\n",
    "\n",
    "Collaborators: Attended TA Hours 4/5, 4/7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "In this homework, we will be exploring a more realistic application of similarity-based leanring. It might be helpful to review **Lab 8 (Feature Transformation and Similarity-based Prediction with k-NN)** first. Most of the things we ask you to do in this homework are explained in the lab. In general, you should feel free to import any package that we have previously used in class. Ensure that all plots have the necessary components that a plot should have (e.g. axes labels, a title, a legend).\n",
    "\n",
    "Furthermore, in addition to recording your collaborators on this homework, please also remember to cite/indicate all external sources used when finishing this assignment. This includes peers, TAs, and links to online sources. Note that these citations will not free you from your obligation to submit your _own_ code and write-ups, however, they will be taken into account during the grading and regrading process.\n",
    "\n",
    "### Submission instructions\n",
    "* Submit this python notebook including your answers in the code cells as homework submission.\n",
    "* **Feel free to add as many cells as you need to** ‚Äî just make sure you don't change what we gave you. \n",
    "* **Does it spark joy?** Note that you will be partially graded on the presentation (_cleanliness, clarity, comments_) of your notebook so make sure you [Marie Kondo](https://lifehacker.com/marie-kondo-is-not-a-verb-1833373654) your notebook before submitting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using `sklearn` for $k$-Nearest Neighbors\n",
    "\n",
    "In Lab 8, we got familiar with $k$-nearest neighbors ($k$-NN) by implementing the algorithm. If you are still not comfortable with how the algorithm works, then we suggest that you review your work from the lab. We will proceed here under this assumption.\n",
    "\n",
    "In this section, we will explore how to use the [$k$-NN _regression_ model supplied by `sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor). You can find the [$k$-NN _classification_ model here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Some Data\n",
    "\n",
    "We'll need to start by getting some data ‚Äî what is data science without data? For this assignment, we will be revisiting another old friend: the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're here, let's review what this dataset is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.1\n",
    "\n",
    "**Write-up!** How many examples are in the dataset? How many features does it have? What are the features? What is the target variable that we would like to estimate? What kind of machine learning problem is this?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "\n",
    "There are 506 examples in the dataset with 13 features each. The features are as follows:\n",
    "\n",
    "- CRIM     per capita crime rate by town\n",
    "- ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS    proportion of non-retail business acres per town\n",
    "- CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- NOX      nitric oxides concentration (parts per 10 million)\n",
    "- RM       average number of rooms per dwelling\n",
    "- AGE      proportion of owner-occupied units built prior to 1940\n",
    "- DIS      weighted distances to five Boston employment centres\n",
    "- RAD      index of accessibility to radial highways\n",
    "- TAX      full-value property-tax rate per $10,000\n",
    "- PTRATIO  pupil-teacher ratio by town\n",
    "- B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "- LSTAT    % lower status of the population\n",
    "- MEDV     Median value of owner-occupied homes in $1000's\n",
    "\n",
    "The target variable that we would like to estimate is the price of houses. This is a supervised machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data\n",
    "\n",
    "In the lab, we also looked at data scaling and transformations. Here we'll demonstrate how to use `sklearn` to help us with this. Let's call this **approach 1**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# new train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "# compute the mean and standard deviation on a training set \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "# apply the transforamtion to both the trainnig and the test set\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative and much quicker way of scaling the the data is the following (let's call this **approach 2**): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41978194,  0.28482986, -1.2879095 , ..., -1.45900038,\n",
       "         0.44105193, -1.0755623 ],\n",
       "       [-0.41733926, -0.48772236, -0.59338101, ..., -0.30309415,\n",
       "         0.44105193, -0.49243937],\n",
       "       [-0.41734159, -0.48772236, -0.59338101, ..., -0.30309415,\n",
       "         0.39642699, -1.2087274 ],\n",
       "       ...,\n",
       "       [-0.41344658, -0.48772236,  0.11573841, ...,  1.17646583,\n",
       "         0.44105193, -0.98304761],\n",
       "       [-0.40776407, -0.48772236,  0.11573841, ...,  1.17646583,\n",
       "         0.4032249 , -0.86530163],\n",
       "       [-0.41500016, -0.48772236,  0.11573841, ...,  1.17646583,\n",
       "         0.44105193, -0.66905833]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = preprocessing.scale(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.2\n",
    "\n",
    "**Write-up** What types of scaling does `StandardScaler()` and `scale` perform? `Hint` Use the [`?` operator](https://ipython.readthedocs.io/en/stable/interactive/python-ipython-diff.html#accessing-help). Which of the two proceedures is a more appropriate preprocessing step for supervised machine learning and _why_? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "StandardScaler() removes the mean and scales to unit variance. \"scale\" standardizes a dataset along any axis; it centers to the mean and scales component wise to unit variance. StandardScaler() is a more appropriate preprocessing step for supervised machine learning because it only fits to the training set, as opposed to fitting to the entire set of data which can risk data snooping (because of the use of the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing.StandardScaler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking Into the Model\n",
    "\n",
    "Now that we have some data to play with, let's try building a $k$-NN regression model. The model provided by `sklearn` shares the a similar interface as the other models that we have looked at previously (esp. $k$-means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.3\n",
    "\n",
    "Use the [`?` operator](https://ipython.readthedocs.io/en/stable/interactive/python-ipython-diff.html#accessing-help) provided by IPython to explore `model` and it's interface.\n",
    "\n",
    "**Try this!** In the cell below, complete the following:\n",
    "1. create and fit a new `KNeighborsRegressor` model with 5 neighbors\n",
    "2. make some predictions using the model on your testing data\n",
    "3. evaluate the performance of the model by computing $R^2$ and storing it in `r_squared`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7153853569196222"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# your code here\n",
    "newModel = KNeighborsRegressor(n_neighbors=5)\n",
    "newModel.fit(X_train, y_train)\n",
    "\n",
    "r_squared = newModel.score(X_test, y_test)\n",
    "assert np.isclose(r_squared, 0.71538, rtol=1e-4), 'You should see this R^2 value'\n",
    "\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.4\n",
    "\n",
    "**Write-up!** What was the $R^2$ value for your $k$-NN model using five neighbors? What does $R^2$ tell you about a model? What does this score tell you about your model?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "An r squared value measures how close the data is to being a fitted regression line. The r squared value for my k-NN model using 5 neighbors is 0.7153853569196222, which tells me that my data is relatively close to a fitted regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, let's move on to some more interesting things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choosing $k$ with Cross Validation\n",
    "\n",
    "In order to test whether the `kNN` algorithm (or any other machine learning algorithm) performs how we want it to and accurately makes predictions, we must compare the known label of all datapoints to the predicted label of those same datapoints. So far we have seen this in the forms of model evaluation and validation in model selection. \n",
    "\n",
    "In model evaluation we partitioned our original dataset into two parts: a training set and a testing set. As we have seen earlier in the course, the testing set is a smaller percentage of the total dataset than the training set.\n",
    "\n",
    "Later on, in model selection, we explored why it was important to have yet another set of data partitioned out for usage as a validation set, which we could use to experiment with a model's hyperparameters. The validation set allowed use to \"evaluate\" our model's performance with various settings of it's parameters while maintaining a completely untouched dataset for out final evaluation.\n",
    "\n",
    "We can extend this idea once again to improve our estimates of model performance through **cross validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `kFolds` method\n",
    "\n",
    "One version of cross validation partitions the dataset into `k` partitions, or folds. We use `k-1` folds to train the model, then the one fold we left out to test the model. We iterate this process `k` times, leaving out a different fold each time, so that we have an accuracy score for each one of the `k` different partitions. We can then take the average of all of these accuracies to calculate a more wholistic accuracy representation of the algorithm. In the example below, `k = 5`; there are 5 partitions. Each partition is used once as a test partition while the other 4 are used for training purposes. The idea for $k$-fold cross validation is based on the realization that we can get a better picture of our model's performance by feeding it many combinations of our data.\n",
    "\n",
    "![](utility/pics/kFold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `KFold` function to partition our dataset into `k` partitions. While the `KFold` function does not split the dataset itself, it provides the indices by which to split the dataset.\n",
    "\n",
    "Below, we split an arbitrary array of length 10 into 5 folds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iteration 0: Train indices: [0 1 2 3 4 6 7 9]. Test indices: [5 8]\n",
      "For iteration 1: Train indices: [0 1 2 4 5 6 8 9]. Test indices: [3 7]\n",
      "For iteration 2: Train indices: [0 1 2 3 5 7 8 9]. Test indices: [4 6]\n",
      "For iteration 3: Train indices: [1 2 3 4 5 6 7 8]. Test indices: [0 9]\n",
      "For iteration 4: Train indices: [0 3 4 5 6 7 8 9]. Test indices: [1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "dummy = np.arange(10) # example data\n",
    "\n",
    "# initialize KFolds\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# iterating over k different splits of dummy\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(dummy)):\n",
    "    print(f'For iteration {fold}: Train indices: {train_idx}. Test indices: {test_idx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how each testing datapoint appears once, ensuring that all datapoints have had a chance to be be tested against the model trained with the rest of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1\n",
    "\n",
    "Now, let's try using the `KFold` operation on the full Boston Housing dataset, building and fitting new $k$-NN models with each fold, and averaging the scores of each model.\n",
    "\n",
    "**Try this!** Complete the `knn_kfolds` function so that it performs `n_folds`-fold cross validation of $k$-NN models on `X` using `n_neighbors` and returns the average $R^2$ value of the models in `avg_score`.\n",
    "\n",
    "* Make sure to scale your training and test sets appropriately (√† la the [Scaling Data](#Scaling-Data) section).\n",
    "* Ensure that you make and fit a new model for each fold.\n",
    "* Also, please make sure that you set `random_state` appropriately in your initialization of `KFold`.\n",
    "\n",
    "`Hint` Refer to the previous example of how to use `KFold` and your work in [Problem 1.2](#Problem-1.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_kfolds(X, y, n_folds, n_neighbors, random_state=None):\n",
    "    '''Computes'''\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "    # initialize KFolds\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    sum = 0\n",
    "    # iterating over k different splits of data\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        newModel = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "        X_train = X[train_idx]\n",
    "        X_test = X[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "        \n",
    "        # compute the mean and standard deviation on a training set \n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        \n",
    "        # apply the transforamtion to both the training and the test set\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)  \n",
    "        \n",
    "        newModel.fit(X_train, y_train)\n",
    "        \n",
    "        sum += newModel.score(X_test, y_test)\n",
    "    avg_score = sum/n_folds\n",
    "    \n",
    "    assert np.isscalar(avg_score), 'The average score should be a single number'\n",
    "    assert 0 <= avg_score and avg_score <= 1, 'The average score should be between 0 and 1'\n",
    "    \n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing $k$\n",
    "\n",
    "We can use cross validation as a substitute for the model selection algorithm that we've used in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.2\n",
    "\n",
    "In this problem, we will use cross validation and our `knn_kfolds` function to help us pick the right $k$ to use for our Boston Housing predictions.\n",
    "\n",
    "**Try this!** In the following cell, use 10-fold cross validation to evaluate the performance of $k$-NN on $X_{\\text{scaled}}$ and $y$ from the Boston Housing dataset and provide a plot of the cross validation average $R^2$ values for $k$ values from 1 to 20 (inclusive). Use a random state of 12 for your analysis. Ensure that your plot has the appropriate components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a18c3dc50>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "import matplotlib.pyplot as plt\n",
    "results = []\n",
    "karray = [i for i in range(1, 21)]\n",
    "for i in range(1, 21):\n",
    "    results.append(knn_kfolds(X_scaled, y, n_folds = 10, n_neighbors = i, random_state=12))\n",
    "plt.plot(karray, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3\n",
    "\n",
    "**Write-up!** Based on your plot from [Problem 2.2](#Problem-2.2), which $k$ value would you pick for your final model? Explain why."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here \n",
    "I would pick a k value of 3 because it is at that point that the r squared value is highest (meaning that at a k value of 3, the model most closely resembles a linear regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Selection with Cross Validation\n",
    "\n",
    "As mentioned before, we can use cross validation to get a more thorough evaluation of model performance. This means that we can use if for model selection by substituting it for the validation set process that we have used in the past.\n",
    "\n",
    "In this section, we will compare our $k$-NN regression model with a linear regression model that we used back in Lab 4 when we last looked at the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1\n",
    "\n",
    "**Try this!** In the following cell, report the cross validation score (average $R^2$) of a $k$-NN model with the $k$ you selected in [Problem 2.3](#Problem-2.3) on `X_scaled`. Use a random state of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8080891806722349"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "knn_kfolds(X_scaled, y, n_folds = 10, n_neighbors = 3, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.2\n",
    "\n",
    "Now let's do 10-fold cross validation on a linear regression model on `X` without scaling.\n",
    "\n",
    "**Write-up** Why should shouldn't we use scaling here? What will happen if we do?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "We can use scaling on the model, but it is not necessary because it will have no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try this!** Perform 10-fold cross validation for linear regression on `X` and report the average $R^2$ value across all of the folds. Make sure to create and fit new models for each fold of the process. Use a random state of 5. `Hint` Refer to your work in [Problem 2.1](#Problem-2.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.719405130574628"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "def linear_kfolds(X, y, n_folds, n_neighbors, random_state=None):\n",
    "#   initialize KFolds\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    sum = 0\n",
    "    # iterating over k different splits of data\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        newModel = linear_model.LinearRegression()\n",
    "        X_train = X[train_idx]\n",
    "        X_test = X[test_idx]\n",
    "        y_train = y[train_idx]\n",
    "        y_test = y[test_idx]\n",
    "        \n",
    "        # compute the mean and standard deviation on a training set \n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        \n",
    "        # apply the transforamtion to both the training and the test set\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)  \n",
    "        \n",
    "        newModel.fit(X_train, y_train)\n",
    "        \n",
    "        sum += newModel.score(X_test, y_test)\n",
    "    avg_score = sum/n_folds\n",
    "    return avg_score\n",
    "linear_kfolds(X_scaled, y, n_folds = 10, n_neighbors = 3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.3\n",
    "\n",
    "**Write-up!** What were the $R^2$ values for each of the models? Which model would you prefer? Why?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "The r squared value was 0.8080891806722349 for the K-Nearest Neighbors model and was 0.719405130574628 for the Linear Regression model. I would prefer the K-Nearest Neighbors model because the r squared value is higher, indicating that a higher portion of the data can be explained by a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.4\n",
    "\n",
    "**Write-up!** Describe your next steps as a data scientist now that you have decided which model to use."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# your response here\n",
    "Now that I have decided which model to use, I can actually use it by running it on all of the data and gathering results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
